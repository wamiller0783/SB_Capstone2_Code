{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naming the Stars</h1>\n",
    "<h2>Using Convoluted Neural Networks to Classify Galaxy Images</h2>\n",
    "\n",
    "<img src=\"./images/starfield.png\" width=300>\n",
    "\n",
    "As of the most recent estimates, there are more than 2 trillion galaxies in the universe. While telescopes are continually capturing images of these galaxies, the sheer number of them makes their classification by humans an unlikely, if not impossible, task. Fortunately, with the creation of neural networks that excel at image classification, this problem is imminently solvable. I set out to see just how accurately galaxies could be classified by a convolutional neural network, applying a pre-trained neural net to images from the Sloan Digital Sky Survey.\n",
    "\n",
    "In 2007, the Sloan Digital Sky Survey published a data set consisting of over a million images of galaxies. These were eventually added to other data sets of galaxy images by a team of scientists known as Galaxy Zoo, which facilitated the crowd-sourced labelling of these galaxies. Much of this data was eventually released as a part of a Kaggle competition in 2013, which provided galaxy images along with the probability of classifications applying to each galaxy. The data resulting from this crowd sourcing can be found [here](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the data\n",
    "One of the initial challenges in obtaining actual galaxy classifications from this data is that the features are determined by answers to questions in a designated task list (pictured below). Each feature in the data set represents a percentage of crowd-sourced data labels that assigned a classification to an image. For instance, if 62% of labellers gave an answer of \"No\" on task 3, a value of 0.62 would be assigned Class 3.2.\n",
    "\n",
    "\n",
    "<img src=\"./images/tasklist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart can, to at least some degree, be distilled into Hubble's galaxy classification system. As can be seen below, this divides galaxies into ellipticals, spirals, and spiral bar classes.\n",
    "<img src=\"./images/hubbleclasses.png\" width=400>\n",
    "\n",
    "\n",
    "In the crowd sourced data, ellipticals would be indicated by class 1.1 in the data.\n",
    "Spirals would be indicated by classes 4.1 and 3.2 together.\n",
    "Spiral bar galaxies would be indicated by classes 4.1 and 3.1 together.\n",
    "\n",
    "Subclasses of each of these would be indicated in classes 7 (degrees of elliptical), or 10 and 11 (types of spiral and spiral bar galaxies).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before immediately training a model to predict to the crowd-sourced data labels, it is worth exploring the shape of that data a bit and verifying its integrity. As a first step, I wanted to ensure that there was actually a high degree of agreement among labellers - if the maximum certainty for most of the classes is no higher than 60%, the data may be mostly useless.\n",
    "\n",
    "\n",
    "<img src=\"./images/maxagreement.png\">\n",
    "\n",
    "Fortunately, as can be seen in the image above, most classes have a maximum agreement of at least 80%, indicating that there are at least some galaxies which had a classification that was generally agreed upon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it would be good to know how much agreement there is for each class.\n",
    "\n",
    "<img src=\"./images/classdistributionbox.png\">\n",
    "\n",
    "Important information that can be gleaned from these graphs, from left to right:\n",
    "<ol>\n",
    "<li>There is nothing \"odd\" about the majority of labelled galaxy images. (Class 6.2 compared to 6.1)</li>\n",
    "<li>More than half of galaxy images include some kind of feature or disk, but a significant range is observed in every quartile (Class 1.2)</li>\n",
    "<li>Around 40% of galaxies could not be disks viewed edge-on, though significant range is observed in every quartile. (Class 2.2 compared to 2.1)</li>\n",
    "<li>Where galaxies are not disks viewed edge-on, most tend not have have a bar (Class 3.2 compared to 3.1)</li>\n",
    "</ol>\n",
    "Overall, the median agreement for each class tends to be rather small, but it appears there are galaxies in most classes for which a significant number of labellers were in agreement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also found it useful to ensure that galaxies which had a high degree of classification agreement appeared as I thought they would. The images below were found by taking the first example that met each of the following criteria:\n",
    "<ol>\n",
    "    <li>Ellipticals: 90% of labellers assign class 1.1</li>\n",
    "    <li>Spirals: 90% of labellers assign both class 4.1 and 3.2</li>\n",
    "    <li>Spiral bars: 90% of labellers assign both class 4.1 and 3.1</li>\n",
    "</ol>\n",
    "\n",
    "Repeating the exact same criteria, except with a threshold of 60%, yields encouragingly similar results, even if the images are somewhat less distinct.\n",
    "\n",
    "<img src=\"./images/galaxyexamples90.png\">\n",
    "\n",
    "\n",
    "<img src=\"./images/galaxyexamples60.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a convolutional neural network - Prerequisites\n",
    "#### Getting started with an Amazon EC2 Server\n",
    "\n",
    "When I first attempted to train a CNN to predict galaxy class labels from images, I quickly realized that my laptop was not going up to the task. Even if I happened to set all of the parameters optimally on the first try, it would have taken roughly 2 days to train for only 10 epochs. I realized that I needed to learn how to harness some external processing power. After doing some research on my options, I turned to Amazon Web Services. This turned out to be a bit of a process, so I will briefly outline the steps I went through below.\n",
    "\n",
    "##### Initial set up\n",
    "I started off by following the steps laid out in [this very helpful guide](https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/). There was one major hitch, however. AWS would not let me select a p2.xlarge instance, which after thoroughly looking into it, was the option I decided I needed. It turned out that my default limit for this type of instance was automatically set at zero, and I had to convince them - over the course of 2 weeks of communication (mostly on my part) - that this was a thing I actually needed for my project. Fortunately, they eventually granted a limit increase to run one p2.xlarge instance. I then completed the guide I linked to above.\n",
    "\n",
    "##### Migrating the project\n",
    "So I finally had the instance up and running, which after all the negotiation, felt like more of an accomplishment than it should have. However, I ran into a roadblock, as I had never needed to migrate data to or from a remote server anymore. After doing a bit of research, I discovered the I should be using the scp command and the syntax of it. \n",
    "\n",
    "The bit of magic that did the job was:  \n",
    "\n",
    "    ‚Å®scp -i AWSKeyFilename.pem -r ubuntu@(instanceaddress):~/remotepath localpath/\n",
    "\n",
    "where \"instance address\" was the text copied in step 6 of the guide I linked to above, and the rest is hopefully obvious. I included the '~/' before \"remotepath\" and \"/\" after \"localpath\" above because these turned out to be important to include in the paths. Thus began the long process of copying tens of thousands of images of galaxies onto my EC2 server.\n",
    "\n",
    "#### Selecting an environment\n",
    "When I attempted to train the model on my laptop, I had decided to use Keras with a Tensorflow backend, and I opted to stick with this choice. Before starting my project on the remote server, it was important to activate this environment (using Python 3.6) by typing \"source activate tensorflow_p36\". I was then ready to start up my jupyter notebook and start refining my project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image processing\n",
    "Having looked through a large number of the galaxy images, I had discovered that they were all centered on the galaxies to be classified and that there was a large amount of irrelevant space around each galaxy. I found that I could crop the images to half the size around the center of each, which would decrease the number of irrelevant artifacts in each image and exponentially decrease training time. I accomplished this with the function below, which I included in my batch generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "def center_crop_images(filepath_list, image_shape_tuple):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Import images contained in filepath_list, read, transform according to image_shape_tuple\n",
    "    \n",
    "    Inputs: \n",
    "        filepath_list = List of filepaths containing images to process\n",
    "        image_shape_tuple = Tuple in the form of (channels, height, width)\n",
    "        scale_factor = The factor by which the resolution of the images should be multiplied\n",
    "    \"\"\"\n",
    "    width, height, channels = image_shape_tuple\n",
    "    # Get count of files in to crop, as this will be the first element in the output array\n",
    "    path_count = len(filepath_list)\n",
    "    # Divide width and height by 2 in order to allow cropping around center\n",
    "    x_scale_unit = int(height/2)\n",
    "    y_scale_unit = int(width/2)\n",
    "    # Create an empty array in the shape of the final output\n",
    "    img_array = np.zeros(shape=(path_count, width, height, channels))\n",
    "    for idx, path in enumerate(filepath_list):\n",
    "        # Read image\n",
    "        img = plt.imread(path)\n",
    "        # Crop image\n",
    "        img = img[x_scale_unit:x_scale_unit*3,\n",
    "                  y_scale_unit:y_scale_unit*3, :]\n",
    "        # Resize image to properly fit cropped dimensions\n",
    "        img = resize(img, (width, height, channels)) # can be used for resolution downscaling if needed\n",
    "        # Add image to the output array\n",
    "        img_array[idx] = img\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><center>Galaxy images before cropping</center><img src=\"./images/beforecropping.png\" width=300></td>\n",
    "    <td><center>The same images after cropping</center><img src=\"./images/aftercropping.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Convolutional Neural Network\n",
    "Now for the more exciting stuff. After doing some research on various CNNs pre-trained for image prediction, I opted to modify a VGG16 architecture with imagenet weights for my neural network. However, I discovered that I did not have the clearest idea of how to link my training targets to data from the images in batches so that the neural net could be trained on it. I then learned how to create a custom batch generation class - the code for which is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchifier:    \n",
    "    \"\"\"\n",
    "    Class for creating training, testing, and validation batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, parent_path):    \n",
    "        # Describe directory structure\n",
    "        self.path = parent_path\n",
    "        self.target_path = 'target_data/'\n",
    "        self.training_path = parent_path + \"train_images/\"\n",
    "        self.testing_path = parent_path + \"test_images/\"\n",
    "        self.validation_path = parent_path + \"validation_images/\"\n",
    "        \n",
    "        \n",
    "        def get_filepaths(directory):\n",
    "            # Converts directory structures to lists of filepaths\n",
    "            return [f for f in os.listdir(directory) if f[-4:] == '.jpg']\n",
    "        \n",
    "        # Convert each directory to a list of filepaths\n",
    "        self.training_img_files = get_filepaths(self.training_path)\n",
    "        self.testing_img_files = get_filepaths(self.testing_path)   \n",
    "        self.validation_img_files = get_filepaths(self.validation_path)\n",
    "         \n",
    "        \n",
    "        def get_targets(self, target_path):\n",
    "            \"\"\"\n",
    "            Description: Separates target data from key values and returns a dictionary of target data.\n",
    "            Inputs: Target data in a csv file.\n",
    "            Outputs: A dictionary of target values with labels as keys and target data as values.\n",
    "            \"\"\"\n",
    "            # Create dataframe from the name of the csv file in target_path\n",
    "            targets_df = pd.read_csv(target_path + 'targets.csv')\n",
    "            # Create empty dictionary for temporary storage of target data\n",
    "            targets = {}\n",
    "            # Iterate through the targets_df dataframe\n",
    "            for idx, row in targets_df.iterrows():\n",
    "                # Create a mask to select only the label data\n",
    "                key_mask = row.index.isin(['GalaxyID'])\n",
    "                # Convert the label data to a string to use as a dictionary key\n",
    "                key = str(int(row['GalaxyID']))\n",
    "                # Use the inverse of the key mask to select only the target data\n",
    "                target_values = row[~key_mask]\n",
    "                # Use the key to store the target values in a particular instance of the targets dictionary\n",
    "                targets[key] = list(target_values.values)\n",
    "            return targets\n",
    "        \n",
    "        # Use the get_targets function to assign target data dictionary to self.targets\n",
    "        self.targets = get_targets(self, target_path)\n",
    "\n",
    "    # Create an ID from the filename of each image\n",
    "    def get_galaxyid(self,fname):\n",
    "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
    "        \n",
    "    # Get the target data assigned to any particular label.\n",
    "    def find_label(self, label):\n",
    "        return self.targets[label]   \n",
    "\n",
    "batch_object = Batchifier(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was then referenced by the generator for each of the training, validation, and test sets as follows (substituting the filepaths and appropriate variable names for each set, of course):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingBatchGenerator(batch):\n",
    "    while 1:\n",
    "        for f in batch.training_img_files:\n",
    "            # Crop image contained in \"f\".\n",
    "            X_train = center_crop_images([batch.training_path + '/' + fname for fname in [f]], (212, 212, 3))\n",
    "            # Get the ID (label) for this image file\n",
    "            galaxyid_ = batch.get_galaxyid(f)\n",
    "            # Get the target data associated with this ID\n",
    "            y_train = np.array(batch.find_label(galaxyid_))\n",
    "            # Ensure the target data is shaped appropriately to the number of classes\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            # Return formatted training and target data.\n",
    "            yield (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created a way to feed data to my neural net, I finalized my network. All of the lines prior to \"flatten\" are part of the VGG16 architecture. To better enable the network to better fit my data, I flattened the output from this architecture, and I added a 1024 neuron dense layer, a dropout layer, another 1024 neuron dense layer, and a final dense layer to set the network output equal to the number of classes. Choosing \"sigmoid\" activation over \"softmax\" in the final layer was very important in this case, as \"softmax\" forces all classes to sum up to one, which is not the case with the classes in the target data. The purpose of the dropout layer was to regularize the network, dropping out random neurons during training to prevent the model from overfitting to the training data.\n",
    "\n",
    "The code to accomplish this was relatively simple, and was written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a VGG model using pretrain imagenet weights, not including the top-most layers\n",
    "vgg = VGG16(weights='imagenet', input_shape = (212,212,3), include_top=False)\n",
    "\n",
    "#Adding custom top layers to allow training for this particular data\n",
    "x = vgg.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "# It is important to use \"sigmoid\" activation in this case, as probabilities for each class do not add to 1.\n",
    "predictions = Dense(37, activation=\"sigmoid\")(x)\n",
    "model_final = Model(inputs = vgg.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final architecture of the network is summarized as:\n",
    "```\n",
    "_____________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 212, 212, 3)       0         \n",
    "_________________________________________________________________\n",
    "block1_conv1 (Conv2D)        (None, 212, 212, 64)      1792      \n",
    "_________________________________________________________________\n",
    "block1_conv2 (Conv2D)        (None, 212, 212, 64)      36928     \n",
    "_________________________________________________________________\n",
    "block1_pool (MaxPooling2D)   (None, 106, 106, 64)      0         \n",
    "_________________________________________________________________\n",
    "block2_conv1 (Conv2D)        (None, 106, 106, 128)     73856     \n",
    "_________________________________________________________________\n",
    "block2_conv2 (Conv2D)        (None, 106, 106, 128)     147584    \n",
    "_________________________________________________________________\n",
    "block2_pool (MaxPooling2D)   (None, 53, 53, 128)       0         \n",
    "_________________________________________________________________\n",
    "block3_conv1 (Conv2D)        (None, 53, 53, 256)       295168    \n",
    "_________________________________________________________________\n",
    "block3_conv2 (Conv2D)        (None, 53, 53, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_conv3 (Conv2D)        (None, 53, 53, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
    "_________________________________________________________________\n",
    "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
    "_________________________________________________________________\n",
    "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
    "_________________________________________________________________\n",
    "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 18432)             0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1024)              18875392  \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1024)              1049600   \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 37)                37925     \n",
    "=================================================================\n",
    "Total params: 34,677,605\n",
    "Trainable params: 34,677,605\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was compiled with the \"Adam\" optimizer, with the learning rate set to $1*10^{-5}$ and the rest of the parameters set to the defaults. I then trained the model for 50 epochs with callbacks for early stopping, checkpoints, and tensorboard. This trained relatively quickly on the AWS image, with a training time well under 3 minutes per epoch. The lowest mean squared error of the model against the validation data was 0.0084 after training for 40 epochs. Evaluating this model against the data I held out for testing gave an MSE of 0.0094. A plot of the training history is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Galaxy Image Classifier Training History\n",
    "<img src=\"./images/traininghistory.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the convolutional neural network (CNN) to a low MSE and evaluated the performance of the network on the testing data, I could now use the testing images to dig a little bit further into how the model predictions compare with the actual data. I repeated much of the same process that I used in exploring that data initially on the predictions generated by the CNN.\n",
    "\n",
    "One thing which stood out as interesting is that the maximum agreement per class tended to be much lower in the predictions than it was in the actual data, as can be seen in the image below.\n",
    "\n",
    "<img src=\"./images/predictionsmaxagreement.png\">\n",
    "\n",
    "This ended up being due to the fact that classes which had higher average percentages of agreement among data labellers (as seen in the \"Class Distribution by Percent Agreement\" plot above) tended to have higher mean squared errors than those which had lower percentages of agreement, as can be seen in the following bar chart.\n",
    "\n",
    "<img src=\"./images/predictionsMSEbyclass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me, knowing about the performance of the neural network is one thing, but what is really interesting is seeing it actually make useful predictions. To see this, I had it make predictions on some images it had not yet seen and took the first samples it returned with high degrees of agreement for the three basic Hubble classes as seen above. The images that follows are from predictions the neural network made on images it had been exposed to in training or validation:\n",
    "\n",
    "<img src=\"./images/predictedgalaxyexamples80.png\">\n",
    "<img src=\"./images/predictedgalaxyexamples60.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
